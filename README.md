# What I Have Read

> Read papers in other fields whenever possible.
> 
> Read papers from other conferences whenever possible.
> 
> Read as many papers as possible.

* [What I Have Read](#what-i-have-read)
  * [Slides](#slides)
     * [Presentation](#presentation)
     * [Paper slides](#paper-slides)
     * [Notes](#notes)
  * [Summarization Papers](./Summarization.md)
  * [Meta Learning Papers](./Meta-Learning.md)
  * [Graph Neural Networks (GNN)](./Graph-Neural-Networks.md)
  * [Generative Adversarial Networks (GAN)](#generative-adversarial-networks-gan)
  * [Knowledge Distillation](#knowledge-distillation)
  * [Pre-train Based](#pre-train-based-1)
  * [Multi Modal](#multi-modal-1)
  * [Blogs](#blogs)

  
## Slides
### Presentation

* [Meta Learning](slides/presentation/Meta%20Learning.pdf)
* [Advanced pre-training language models a brief introduction](slides/presentation/Advanced%20pre-training%20language%20models%20a%20brief%20introduction.pdf)
* [Graph Neural Networks](slides/presentation/Graph%20Neural%20Networks.pdf)

### Paper slides
* [ACL19-Dynamically Fused Graph Network for Multi-hop Reasoning](slides/paper-slides/Dynamically%20Fused%20Graph%20Network%20for%20Multi-hop%20Reasoning.pdf)
* [NAACL19-Linguistic Knowledge and Transferability of Contextual Representations](slides/paper-slides/Linguistic%20Knowledge%20and%20Transferability%20of%20Contextual%20Representations.pdf)
* [NAACL19-Text Generation from Knowledge Graphs with Graph Transformers](slides/paper-slides/Text%20Generation%20from%20Knowledge%20Graphs%20with%20Graph%20Transformers.pdf)
* [The Curious Case of Neural Text Degeneration ](slides/paper-slides/The%20Curious%20Case%20of%20Neural%20Text%20Degeneration.pdf)
* [EMNLP18-Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination](slides/paper-slides/Multi-Domain%20Neural%20Machine%20Translation%20with%20Word-Level%20Domain%20Context%20Discrimination.pdf)
* [EMNLP18-Commonsense for Generative Multi-Hop Question Answering Tasks](slides/paper-slides/Commonsense%20for%20Generative%20Multi-Hop%20Question%20Answering%20Tasks.pdf)
* [IJCAI18-Commonsense Knowledge Aware Conversation Generation with Graph Attention](slides/paper-slides/Commonsense%20Knowledge%20Aware%20Conversation%20Generation%20with%20Graph%20Attention.pdf)
* [AAAI18-Emotional Chatting Machine Emotional Conversation Generation with Internal and External Memory](slides/paper-slides/Emotional%20Chatting%20Machine%20Emotional%20Conversation%20Generation%20with%20Internal%20and%20External%20Memory.pdf)
* [EMNLP18-Learning Neural Templates for Text Generation](slides/paper-slides/Learning%20Neural%20Templates%20for%20Text%20Generation.pdf)
* [ACL18-Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders ](slides/paper-slides/Learning%20to%20Ask%20Questions%20in%20Open-domain%20Conversational%20Systems%20with%20Typed%20Decoders%20.pdf)
* [ACL17-Semi-Supervised QA with Generative Domain-Adaptive Nets](slides/paper-slides/Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets.pdf)
* [ACL17-Deep Multitask Learning for Semantic Dependency Parsing](slides/paper-slides/Deep%20Multitask%20Learning%20for%20Semantic%20Dependency%20Parsing.pdf)

### Notes
* [GAN in Text Generation](slides/notes/GAN%20in%20Text%20Generation.pdf)
* [Boosting](slides/notes/Boosting.pdf)
* [HMM](slides/notes/HMM.pdf)
* [The Maximum Entropy Model](slides/notes/The%20Maximum%20Entropy%20Model.pdf)
  




## Generative Adversarial Networks (GAN)

| Paper | Name | Conference |
| :---: | :---: | :---: |
|RELGAN: RELATIONAL GENERATIVE ADVERSARIAL NETWORKS FOR TEXT GENERATION|RelGAN|ICLR19|


## Knowledge Distillation
| Paper | Conference |
| :---: | :---: |
|Distilling Task-Speciﬁc Knowledge from BERT into Simple Neural Networks||
|Exploiting the Ground-Truth: An Adversarial Imitation Based Knowledge Distillation Approach for Event Detection|AAAI19|
|On-Device Neural Language Model based Word Prediction|COLING18|
|Cross-lingual Distillation for Text Classiﬁcation|ACL17|
|Sequence-Level Knowledge Distillation|EMNLP16|




## Pre-train Based
| Paper | Conference |
| :---: | :---: |
|XLNet: Generalized Autoregressive Pretraining for Language Understanding||
|Pre-Training with Whole Word Masking for Chinese BERT||
|Unified Language Model Pre-training for Natural Language Understanding and Generation||
|ERNIE: Enhanced Representation through Knowledge Integration||
|ERNIE: Enhanced Language Representation with Informative Entities|ACL19|
|MASS: Masked Sequence to Sequence Pre-training for Language Generation|ICML19|
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|NAACL19|
|Linguistic Knowledge and Transferability of Contextual Representations|NAACL19|
|Improving Language Understanding by Generative Pre-Training||
|Deep contextualized word representations|NAACL18|


## Multi Modal
| Paper | Conference |
| :---: | :---: |
|VATEX:A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research||
|Multimodal Abstractive Summarization for How2 Videos|ACL19|
|How2:A Large-scale Dataset for Multimodal Language Understanding|NIPS18|


## Blogs
* [The Why, When, and How of Using Python Multi-threading and Multi-Processing](https://medium.com/towards-artificial-intelligence/the-why-when-and-how-of-using-python-multi-threading-and-multi-processing-afd1b8a8ecca)























